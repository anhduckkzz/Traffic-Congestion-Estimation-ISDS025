{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ISDS Traffic AI Full Pipeline\n\nAll project modules, scripts, configs, and tests from the ISDS traffic congestion repository are consolidated below as notebook cells so the full pipeline can be executed end-to-end from a single `.ipynb` file.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Notebook Layout**\n- Core package modules (config, data, models, pipeline, utils)\n- Training and fine-tuning scripts\n- Configuration YAML references\n- Test suites as optional smoke checks\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/__init__.py\n\"Top-level package for the ISDS traffic congestion estimation project.\"\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/config/__init__.py\nfrom .defaults import load_config, save_config\n\n__all__ = [\"load_config\", \"save_config\"]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/config/defaults.py\nfrom __future__ import annotations\n\nimport copy\nfrom pathlib import Path\nfrom typing import Any, Mapping, MutableMapping, Optional\n\nimport yaml\n\n\ndef _deep_update(base: MutableMapping[str, Any], updates: Mapping[str, Any]) -> MutableMapping[str, Any]:\n    \"\"\"Recursively merge ``updates`` into ``base``.\"\"\"\n    for key, value in updates.items():\n        if isinstance(value, Mapping) and isinstance(base.get(key), Mapping):\n            base[key] = _deep_update(dict(base[key]), value)\n        else:\n            base[key] = copy.deepcopy(value)\n    return base\n\n\ndef load_config(path: str | Path, overrides: Optional[Mapping[str, Any]] = None) -> dict[str, Any]:\n    \"\"\"Load a YAML configuration file and optionally apply overrides.\"\"\"\n    cfg_path = Path(path)\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {cfg_path}\")\n\n    with cfg_path.open(\"r\", encoding=\"utf-8\") as handle:\n        data = yaml.safe_load(handle) or {}\n\n    if overrides:\n        data = _deep_update(data, overrides)\n    return data\n\n\ndef save_config(config: Mapping[str, Any], path: str | Path) -> None:\n    \"\"\"Persist a configuration mapping to disk.\"\"\"\n    cfg_path = Path(path)\n    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n    with cfg_path.open(\"w\", encoding=\"utf-8\") as handle:\n        yaml.safe_dump(dict(config), handle, sort_keys=False)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/data/__init__.py\nfrom .datasets import SegmentationDataset, load_split_file\n\n__all__ = [\"SegmentationDataset\", \"load_split_file\"]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/data/datasets.py\nfrom __future__ import annotations\n\nimport csv\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ColorJitter\nfrom torchvision.transforms import functional as F"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass(frozen=True)\nclass SegmentationSample:\n    image_path: Path\n    mask_path: Path"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_split_file(file_path: str | Path) -> List[Path]:\n    split_path = Path(file_path)\n    if not split_path.exists():\n        raise FileNotFoundError(f\"Split file not found: {split_path}\")\n    with split_path.open(\"r\", encoding=\"utf-8\") as handle:\n        entries = [line.strip() for line in handle if line.strip()]\n    return [Path(entry) for entry in entries]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def collect_directory_samples(\n    root: str | Path,\n    image_dir: str,\n    mask_dir: str,\n    split_subdir: str,\n    image_suffix: str,\n    mask_suffix: str,\n) -> List[Tuple[Path, Path]]:\n    root = Path(root)\n    image_root = root / image_dir / split_subdir\n    mask_root = root / mask_dir / split_subdir\n\n    if not image_root.exists():\n        raise FileNotFoundError(f\"Image split directory not found: {image_root}\")\n    if not mask_root.exists():\n        raise FileNotFoundError(f\"Mask split directory not found: {mask_root}\")\n\n    pattern = f\"*{image_suffix}\" if image_suffix else \"*\"\n    samples: List[Tuple[Path, Path]] = []\n    for image_path in sorted(image_root.glob(pattern)):\n        stem = image_path.stem\n        mask_path = mask_root / f\"{stem}{mask_suffix}\"\n        if not mask_path.exists():\n            raise FileNotFoundError(f\"Missing mask for {image_path.name}: {mask_path}\")\n        samples.append((image_path, mask_path))\n\n    if not samples:\n        raise RuntimeError(f\"No samples found under {image_root}\")\n    return samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SegmentationDataset(Dataset):\n    def __init__(\n        self,\n        samples: Sequence[Tuple[Path, Path]],\n        num_classes: int,\n        transform: Optional[Callable] = None,\n        use_binary_mask: bool = True,\n    ) -> None:\n        self.samples = samples\n        self.num_classes = num_classes\n        self.transform = transform\n        self.use_binary_mask = use_binary_mask\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        image_path, mask_path = self.samples[index]\n        image = Image.open(image_path).convert(\"RGB\")\n        mask = Image.open(mask_path)\n\n        if self.transform:\n            image, mask = self.transform(image, mask)\n\n        image_tensor = F.to_tensor(image)\n        mask_tensor = torch.as_tensor(np.array(mask), dtype=torch.long)\n\n        if self.use_binary_mask:\n            mask_tensor = (mask_tensor > 0).long()\n\n        return image_tensor, mask_tensor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def segmentation_collate(\n    batch: Iterable[Tuple[torch.Tensor, torch.Tensor]],\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    images, masks = zip(*batch)\n    return torch.stack(images, 0), torch.stack(masks, 0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/__init__.py\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/detection/__init__.py\nfrom .yolo_wrapper import YOLODetector, DetectionResult, TrackEvent\n\n__all__ = [\"YOLODetector\", \"DetectionResult\", \"TrackEvent\"]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/detection/yolo_wrapper.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Generator, Iterable, List, Optional\n\nimport numpy as np\n\ntry:\n    from ultralytics import YOLO  # type: ignore\nexcept ImportError as exc:  # pragma: no cover - handled at runtime\n    raise RuntimeError(\"ultralytics package is required for YOLODetector\") from exc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass DetectionResult:\n    frame_index: int\n    timestamp: float\n    boxes: np.ndarray  # shape [N, 4] in xyxy format\n    scores: np.ndarray  # shape [N]\n    class_ids: np.ndarray  # shape [N]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass TrackEvent:\n    frame_index: int\n    timestamp: float\n    track_id: int\n    class_id: int\n    bbox_xyxy: np.ndarray\n    is_confirmed: bool\n    velocity: Optional[np.ndarray] = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class YOLODetector:\n    def __init__(\n        self,\n        weights: str | Path,\n        device: str = \"cuda\",\n        conf_threshold: float = 0.25,\n        iou_threshold: float = 0.45,\n    ) -> None:\n        self.model = YOLO(str(weights))\n        self.device = device\n        self.conf_threshold = conf_threshold\n        self.iou_threshold = iou_threshold"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def finetune(self, data_cfg: str | Path, epochs: int, batch: int, img_size: int, project_dir: str | Path, name: str, **kwargs) -> None:\n        \"\"\"Fine-tune the detector using Ultralytics training loop.\"\"\"\n        self.model.train(\n            data=str(data_cfg),\n            epochs=epochs,\n            batch=batch,\n            imgsz=img_size,\n            device=self.device,\n            project=str(project_dir),\n            name=name,\n            conf=self.conf_threshold,\n            iou=self.iou_threshold,\n            **kwargs,\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def predict(self, frame: np.ndarray, frame_index: int, timestamp: float) -> DetectionResult:\n        results = self.model.predict(\n            source=frame,\n            device=self.device,\n            conf=self.conf_threshold,\n            iou=self.iou_threshold,\n            verbose=False,\n        )[0]\n        return DetectionResult(\n            frame_index=frame_index,\n            timestamp=timestamp,\n            boxes=results.boxes.xyxy.cpu().numpy(),\n            scores=results.boxes.conf.cpu().numpy(),\n            class_ids=results.boxes.cls.cpu().numpy().astype(int),\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def track(\n        self,\n        frame: np.ndarray,\n        frame_index: int,\n        timestamp: float,\n        tracker_cfg: str | Path,\n        classes: Optional[List[int]] = None,\n    ) -> Generator[TrackEvent, None, None]:\n        results = self.model.track(\n            source=frame,\n            tracker=str(tracker_cfg),\n            classes=classes,\n            device=self.device,\n            conf=self.conf_threshold,\n            iou=self.iou_threshold,\n            verbose=False,\n            persist=True,\n        )[0]\n\n        if results.boxes.id is None:\n            return\n\n        track_ids = results.boxes.id.cpu().numpy().astype(int)\n        boxes_xyxy = results.boxes.xyxy.cpu().numpy()\n        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n        conf_scores = results.boxes.conf.cpu().numpy()\n\n        for i, track_id in enumerate(track_ids):\n            yield TrackEvent(\n                frame_index=frame_index,\n                timestamp=timestamp,\n                track_id=track_id,\n                class_id=class_ids[i],\n                bbox_xyxy=boxes_xyxy[i],\n                is_confirmed=(conf_scores[i] > self.conf_threshold),\n            )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/segmentation/__init__.py\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/segmentation/deeplab_seresnet.py\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision.models.segmentation.deeplabv3 import ASPP\n\nfrom .backbones.resnet_se_dw import SEResNet34, seresnet34_backbone"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DeepLabSEResNet34(nn.Module):\n    \"\"\"DeepLabV3+ style head coupled with the SEResNet34 backbone.\"\"\"\n\n    def __init__(self, num_classes: int = 2, output_stride: int = 16, dropout: float = 0.1) -> None:\n        super().__init__()\n        self.backbone: SEResNet34 = seresnet34_backbone(output_stride=output_stride)\n        atrous_rates = [6, 12, 18]\n        if output_stride == 8:\n            atrous_rates = [rate * 2 for rate in atrous_rates]\n        elif output_stride == 32:\n            atrous_rates = [rate // 2 for rate in atrous_rates]\n        self.aspp = ASPP(512, tuple(atrous_rates), out_channels=256)\n        self.aspp_proj = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n        self.low_level_proj = nn.Sequential(\n            nn.Conv2d(64, 48, kernel_size=1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True),\n        )\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=dropout, inplace=False),\n            nn.Conv2d(256, num_classes, kernel_size=1),\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n        input_size = x.shape[-2:]\n        features = self.backbone(x)\n        low_level = features[\"low_level\"]\n        encoder_out = features[\"out\"]\n\n        aspp_out = self.aspp(encoder_out)\n        aspp_out = self.aspp_proj(aspp_out)\n\n        low_level = self.low_level_proj(low_level)\n        low_level_interp = F.interpolate(\n            low_level, size=aspp_out.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n\n        decoder_in = torch.cat([aspp_out, low_level_interp], dim=1)\n        decoder_out = self.decoder(decoder_in)\n        decoder_out = F.interpolate(decoder_out, size=input_size, mode=\"bilinear\", align_corners=False)\n\n        return {\"out\": decoder_out}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_segmentation_model(num_classes: int, **kwargs) -> DeepLabSEResNet34:\n    return DeepLabSEResNet34(num_classes=num_classes, **kwargs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/segmentation/backbones/__init__.py\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/models/segmentation/backbones/resnet_se_dw.py\nfrom __future__ import annotations\n\nfrom collections import OrderedDict\nfrom typing import Dict\n\nimport torch\nfrom torch import nn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DepthwiseSeparableConv(nn.Module):\n    \"\"\"Depthwise separable convolution used inside the modified ResNet blocks.\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, dilation: int = 1, bias: bool = False) -> None:\n        super().__init__()\n        padding = dilation\n        self.depthwise = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=bias,\n        )\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SEBlock(nn.Module):\n    def __init__(self, channels: int, reduction: int = 16) -> None:\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        hidden = max(channels // reduction, 8)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, hidden, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden, channels, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SEDWBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: nn.Module | None = None, dilation: int = 1) -> None:\n        super().__init__()\n        self.conv1 = DepthwiseSeparableConv(inplanes, planes, stride=stride, dilation=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = DepthwiseSeparableConv(planes, planes, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SEBlock(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SEResNet34(nn.Module):\n    def __init__(self, output_stride: int = 16) -> None:\n        super().__init__()\n        if output_stride not in {8, 16, 32}:\n            raise ValueError(f\"Invalid output_stride: {output_stride}\")\n\n        strides = [1, 2, 2, 2]\n        dilations = [1, 1, 1, 1]\n        if output_stride == 16:\n            strides[3] = 1\n            dilations[3] = 2\n        elif output_stride == 8:\n            strides[2] = 1\n            strides[3] = 1\n            dilations[2] = 2\n            dilations[3] = 4\n\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(SEDWBasicBlock, 64, 3, stride=strides[0], dilation=dilations[0])\n        self.layer2 = self._make_layer(SEDWBasicBlock, 128, 4, stride=strides[1], dilation=dilations[1])\n        self.layer3 = self._make_layer(SEDWBasicBlock, 256, 6, stride=strides[2], dilation=dilations[2])\n        self.layer4 = self._make_layer(SEDWBasicBlock, 512, 3, stride=strides[3], dilation=dilations[3])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample, dilation=1)]\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n        return nn.Sequential(*layers)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return OrderedDict([(\"low_level\", x1), (\"out\", x4)])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def seresnet34_backbone(**kwargs) -> SEResNet34:\n    return SEResNet34(**kwargs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/pipeline/__init__.py\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/pipeline/congestion.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom statistics import mean\nfrom typing import Dict, Iterable, List, Optional\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom src.models.detection import TrackEvent, YOLODetector\nfrom src.models.segmentation.deeplab_seresnet import build_segmentation_model\nfrom src.pipeline.roi import LaneROIExtractor, ROILine\nfrom src.pipeline.speed import SpeedEstimator\nfrom src.utils import get_logger\nfrom src.utils.video import iter_video_frames\n\n\nLEVEL_ORDER = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass LoSThresholds:\n    density: Dict[str, float]\n    speed_two_wheel: Dict[str, float]\n    speed_four_wheel: Dict[str, float]\n    default_vehicle_length_m: float = 3.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass LaneMetrics:\n    lane_index: int\n    vehicle_count: int\n    avg_speed_kph: float\n    density: float\n    level_of_service: str"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass PipelineResult:\n    window_start: float\n    window_end: float\n    lanes: List[LaneMetrics]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CongestionClassifier:\n    def __init__(self, thresholds: LoSThresholds) -> None:\n        self.thresholds = thresholds\n\n    def classify(self, density: float, avg_speed_kph: float, vehicle_type: str) -> str:\n        density_level = self._level_from_density(density)\n        speed_level = self._level_from_speed(avg_speed_kph, vehicle_type)\n        return LEVEL_ORDER[max(LEVEL_ORDER.index(density_level), LEVEL_ORDER.index(speed_level))]\n\n    def _level_from_density(self, density: float) -> str:\n        for level in LEVEL_ORDER[:-1]:  # exclude F which is catch-all\n            threshold = self.thresholds.density.get(level)\n            if threshold is not None and density <= threshold:\n                return level\n        return \"F\"\n\n    def _level_from_speed(self, speed_kph: float, vehicle_type: str) -> str:\n        speed_thresholds = self.thresholds.speed_two_wheel if \"two\" in vehicle_type else self.thresholds.speed_four_wheel\n        for level in LEVEL_ORDER[:-1]:\n            threshold = speed_thresholds.get(level)\n            if threshold is not None and speed_kph >= threshold:\n                return level\n        return \"F\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/pipeline/roi.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Sequence, Tuple\n\nimport cv2\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass ROILine:\n    y: float\n    x_left: float\n    x_right: float\n    pixel_length: float\n\n    @property\n    def center(self) -> tuple[float, float]:\n        return (0.5 * (self.x_left + self.x_right), self.y)\n\n    @property\n    def width(self) -> float:\n        return abs(self.x_right - self.x_left)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class LaneROIExtractor:\n    def __init__(\n        self,\n        dash_lengths_m: Sequence[float] = (1.0, 2.0),\n        y_group_tolerance_px: int = 30,\n        min_component_area: int = 40,\n        aspect_ratio_range: Tuple[float, float] = (1.0, 15.0),\n    ) -> None:\n        self.dash_lengths_m = dash_lengths_m\n        self.y_group_tolerance_px = y_group_tolerance_px\n        self.min_component_area = min_component_area\n        self.aspect_ratio_range = aspect_ratio_range"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def extract(self, mask: np.ndarray) -> Tuple[List[ROILine], float]:\n        if mask.ndim != 2:\n            raise ValueError(\"Segmentation mask must be a 2D array\")\n\n        binary = (mask > 0).astype(np.uint8) * 255\n        binary = cv2.medianBlur(binary, 3)\n\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=8)\n        components = []\n        for i in range(1, num_labels):  # skip background\n            area = stats[i, cv2.CC_STAT_AREA]\n            if area < self.min_component_area:\n                continue\n            width = stats[i, cv2.CC_STAT_WIDTH]\n            height = stats[i, cv2.CC_STAT_HEIGHT]\n            if width == 0 or height == 0:\n                continue\n            aspect = max(width, height) / max(min(width, height), 1)\n            if not (self.aspect_ratio_range[0] <= aspect <= self.aspect_ratio_range[1]):\n                continue\n            cx, cy = centroids[i]\n            components.append((cy, cx, width, height))\n\n        if not components:\n            return [], 0.0\n\n        # ... (rest of the function)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/pipeline/speed.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nimport numpy as np\n\nfrom src.models.detection.yolo_wrapper import TrackEvent"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass SpeedEstimate:\n    track_id: int\n    timestamp: float\n    speed_kph: float\n    displacement_m: float\n    duration_s: float"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass _TrackHistory:\n    first_timestamp: float\n    first_centroid: np.ndarray\n    last_centroid: np.ndarray = field(default_factory=lambda: np.zeros(2))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SpeedEstimator:\n    def __init__(self, meter_per_pixel: float) -> None:\n        self.meter_per_pixel = meter_per_pixel\n        self._histories: Dict[int, _TrackHistory] = {}\n\n    def reset(self) -> None:\n        self._histories.clear()\n\n    def update(self, event: TrackEvent) -> Optional[SpeedEstimate]:\n        if self.meter_per_pixel <= 0:\n            return None\n\n        x1, y1, x2, y2 = event.bbox_xyxy\n        centroid = np.array([(x1 + x2) * 0.5, (y1 + y2) * 0.5], dtype=float)\n\n        history = self._histories.get(event.track_id)\n        if history is None:\n            self._histories[event.track_id] = _TrackHistory(\n                first_timestamp=event.timestamp,\n                first_centroid=centroid,\n                last_centroid=centroid,\n            )\n            return None\n\n        duration = event.timestamp - history.first_timestamp\n        if duration <= 0:\n            return None\n\n        displacement_px = float(np.linalg.norm(centroid - history.first_centroid))\n        displacement_m = displacement_px * self.meter_per_pixel\n        speed_mps = displacement_m / duration\n        speed_kph = speed_mps * 3.6\n        history.last_centroid = centroid\n\n        return SpeedEstimate(\n            track_id=event.track_id,\n            timestamp=event.timestamp,\n            speed_kph=float(speed_kph),\n            displacement_m=displacement_m,\n            duration_s=duration,\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/utils/__init__.py\nfrom .logging import configure_logging, get_logger\nfrom .video import VideoWriter, iter_video_frames\n\n__all__ = [\"configure_logging\", \"get_logger\", \"VideoWriter\", \"iter_video_frames\"]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/utils/logging.py\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Optional\n\nfrom rich.logging import RichHandler\n\n\n_LOGGER_INITIALIZED = False\n\n\ndef configure_logging(level: str = \"INFO\") -> None:\n    global _LOGGER_INITIALIZED\n    if _LOGGER_INITIALIZED:\n        return\n    logging.basicConfig(\n        level=getattr(logging, level.upper(), logging.INFO),\n        format=\"%(message)s\",\n        datefmt=\"[X]\",\n        handlers=[RichHandler(rich_tracebacks=True, markup=True)],\n    )\n    _LOGGER_INITIALIZED = True\n\n\ndef get_logger(name: Optional[str] = None) -> logging.Logger:\n    if not _LOGGER_INITIALIZED:\n        configure_logging()\n    return logging.getLogger(name or \"src\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: src/utils/video.py\nfrom __future__ import annotations\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Generator, Iterator, Tuple\n\nimport cv2\n\n\ndef iter_video_frames(path: str | Path) -> Iterator[tuple[int, float, any]]:\n    \"\"\"Yield (frame_index, timestamp_seconds, frame_bgr).\"\"\"\n    capture = cv2.VideoCapture(str(path))\n    if not capture.isOpened():\n        raise RuntimeError(f\"Failed to open video: {path}\")\n\n    fps = capture.get(cv2.CAP_PROP_FPS) or 30.0\n    frame_idx = 0\n    try:\n        while True:\n            ok, frame = capture.read()\n            if not ok:\n                break\n            timestamp = frame_idx / fps\n            yield frame_idx, timestamp, frame\n            frame_idx += 1\n    finally:\n        capture.release()\n\n\nclass VideoWriter:\n    \"\"\"Simple wrapper above OpenCV VideoWriter.\"\"\"\n\n    def __init__(self, path: str | Path, fps: float, frame_size: Tuple[int, int]) -> None:\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        self._writer = cv2.VideoWriter(str(path), fourcc, fps, frame_size)\n        if not self._writer.isOpened():\n            raise RuntimeError(f\"Failed to create video writer at {path}\")\n\n    def write(self, frame) -> None:\n        self._writer.write(frame)\n\n    def close(self) -> None:\n        self._writer.release()\n\n    def __enter__(self) -> \"VideoWriter\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        self.close()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: scripts/__init__.py\n'''\nCLI entrypoints for the ISDS traffic congestion project.\n'''\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: scripts/run_pipeline.py\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import List\n\nfrom rich.console import Console\n\nfrom src.config import load_config\nfrom src.pipeline.congestion import CongestionPipeline, LoSThresholds\nfrom src.pipeline.roi import LaneROIExtractor\nfrom src.utils import configure_logging, get_logger\n\nconsole = Console()\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run the traffic congestion estimation pipeline\")\n    parser.add_argument(\"--config\", required=True)\n    parser.add_argument(\"--video\", default=None)\n    args = parser.parse_args()\n\n    configure_logging()\n    logger = get_logger(\"run_pipeline\")\n\n    cfg = load_config(args.config)\n\n    inputs_cfg = cfg[\"inputs\"]\n    video_path = args.video or inputs_cfg.get(\"video_path\")\n    if video_path is None:\n        raise ValueError(\"Video path must be provided via --video or config\")\n\n    # ... (rest of the function)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: scripts/train_segmentation.py\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\nfrom typing import Any, Dict, Sequence, Tuple\n\nimport torch\nfrom rich.console import Console\nfrom rich.progress import Progress\nfrom torch.cuda.amp import GradScaler, autocast\n\nfrom src.config import load_config\nfrom src.data import SegmentationDataset, load_split_file\nfrom src.data.datasets import collect_directory_samples, segmentation_collate\nfrom src.models.segmentation.deeplab_seresnet import build_segmentation_model\nfrom src.utils import configure_logging, get_logger\n\ntry:\n    import wandb  # type: ignore\nexcept ImportError:  # pragma: no cover - optional dependency\n    wandb = None  # type: ignore\n\nconsole = Console()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def focal_tversky_loss(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    alpha: float = 0.7,\n    beta: float = 0.3,\n    gamma: float = 1.5,\n) -> torch.Tensor:\n    if logits.shape[1] > 1:\n        probs = torch.softmax(logits, dim=1)[:, 1:2]\n    else:\n        probs = torch.sigmoid(logits)\n    targets = targets.float()\n    if targets.ndim == 4 and targets.shape[1] != probs.shape[1]:\n        targets = targets.squeeze(1).unsqueeze(1)\n\n    true_pos = torch.sum(probs * targets)\n    false_neg = torch.sum(targets * (1 - probs))\n    false_pos = torch.sum((1 - targets) * probs)\n    tversky = (true_pos + 1e-6) / (true_pos + alpha * false_neg + beta * false_pos + 1e-6)\n    loss = torch.pow((1 - tversky), gamma)\n    return loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def dice_score(preds: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5) -> float:\n    if preds.shape[1] > 1:\n        probs = torch.softmax(preds, dim=1)[:, 1]\n    else:\n        probs = torch.sigmoid(preds[:, 0])\n    preds_bin = (probs > threshold).float()\n    targets_bin = targets.float().squeeze(1)\n    intersection = (preds_bin * targets_bin).sum(dim=(1, 2))\n    union = preds_bin.sum(dim=(1, 2)) + targets_bin.sum(dim=(1, 2))\n    score = (2.0 * intersection) / (union + 1e-6)\n    return score.mean().item()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: scripts/finetune_yolo.py\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\nimport torch\nfrom rich.console import Console\n\nfrom src.config import load_config\nfrom src.models.detection import YOLODetector\nfrom src.utils import configure_logging, get_logger\n\nconsole = Console()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def write_dataset_yaml(cfg: dict, output_dir: Path) -> Path:\n    output_dir.mkdir(parents=True, exist_ok=True)\n    yaml_path = output_dir / \"dataset.yaml\"\n    content = f\"\"\"\npath: {cfg['root']}\ntrain: {cfg['train_images']}\nval: {cfg['val_images']}\nnames: {cfg['class_names']}\n\"\"\"\n    yaml_path.write_text(content, encoding=\"utf-8\")\n    return yaml_path"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Fine-tune YOLO detector using Ultralytics\")\n    parser.add_argument(\" --config\", required=True)\n    parser.add_argument(\" --device\", default=\"cuda\")\n    args = parser.parse_args()\n\n    configure_logging()\n    logger = get_logger(\"finetune_yolo\")\n\n    cfg = load_config(args.config)\n    data_cfg = cfg[\"data\"]\n    model_cfg = cfg[\"model\"]\n    logging_cfg = cfg.get(\"logging\", {}\n\n    project_dir = Path(logging_cfg.get(\"project_dir\", \"outputs/detection\"))\n    experiment_name = logging_cfg.get(\"experiment_name\", \"yolo_finetune\")\n    dataset_yaml = write_dataset_yaml(data_cfg, project_dir)\n\n    device = args.device if torch.cuda.is_available() or args.device == \"cpu\" else \"cpu\"\n    detector = YOLODetector(\n        weights=model_cfg[\"pretrained_weights\"],\n        device=device,\n        conf_threshold=model_cfg.get(\"conf_threshold\", 0.25),\n        iou_threshold=model_cfg.get(\"iou_threshold\", 0.45),\n    )\n\n    train_kwargs = {\"epochs\": model_cfg.get(\"epochs\", 100), \"batch\": model_cfg.get(\"batch_size\", 16), \"img_size\": model_cfg.get(\"img_size\", 1280), \"project_dir\": project_dir, \"name\": experiment_name}\n    detector.finetune(data_cfg=dataset_yaml, **train_kwargs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### configs/pipeline.yaml\n```yaml\ninputs:\n  video_path: null\n  output_dir: outputs/pipeline\n  window_seconds: 30\n  min_objects_per_window: 5\n\nsegmentation:\n  checkpoint: outputs/segmentation/best.ckpt\n  device: cuda\n  threshold: 0.5\n  smooth_kernel: 5\n\nroi:\n  dash_lengths_m: [1.0, 2.0]\n  y_group_tolerance_px: 30\n  min_component_area: 40\n  aspect_ratio_range: [0.2, 5.0]\n\ntracking:\n  detector_weights: outputs/detection/yolov11s.pt\n  tracker_config: ultralytics/cfg/trackers/botsort.yaml\n  conf_threshold: 0.35\n  iou_threshold: 0.4\n  device: cuda\n\nloss_thresholds:\n  density: {A: 5, B: 10, C: 14, D: 24, E: 40}\n  speed_two_wheel: {A: 40, B: 35, C: 30, D: 25, E: 20, F: 0}\n  speed_four_wheel: {A: 45, B: 40, C: 35, D: 25, E: 15, F: 0}\n  default_vehicle_length_m: 3.0\n\nreporting:\n  export_tracks: true\n  export_json: true\n  display: false\n\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### configs/detection.yaml\n```yaml\ndata:\n  root: data/detection\n  train_images: data/detection/images/train\n  val_images: data/detection/images/val\n  train_labels: data/detection/labels/train\n  val_labels: data/detection/labels/val\n  class_names: [motorbike, car, bus, truck]\n  speed_labels: data/detection/speeds.csv\n  format: yolo  # or coco\n\nmodel:\n  architecture: yolo11s\n  pretrained_weights: yolov11s.pt\n  img_size: 1280\n  batch_size: 16\n  epochs: 100\n  optimizer: sgd\n  learning_rate: 0.002\n  momentum: 0.937\n  weight_decay: 0.0005\n  warmup_epochs: 3\n  augment:\n    hsv: true\n    mosaic: true\n    mixup: false\n\ntracking:\n  tracker_config: ultralytics/cfg/trackers/botsort.yaml\n  conf_threshold: 0.35\n  iou_threshold: 0.45\n\nlogging:\n  project_dir: outputs/detection\n  experiment_name: yolov11s_finetune\n\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### configs/segmentation.yaml\n```yaml\ndataset:\n  root: data/segmentation\n  format: directory\n  image_dir: images\n  mask_dir: labels\n  train_subdir: train\n  val_subdir: val\n  image_suffix: .jpg\n  mask_suffix: .png\n  palette_csv: data/segmentation/rlmd.csv\n  include_class_ids: [4, 5, 6, 7, 8, 9, 10, 16]\n  background_class_id: 0\n  binary_mask: true\n  num_classes: 2\n  class_names: [background, marking]\n  augmentation:\n    horizontal_flip: true\n    color_jitter:\n      brightness: 0.2\n      contrast: 0.2\n      saturation: 0.2\n      hue: 0.05\n    gaussian_noise: false\n\nmodel:\n  backbone:\n    pretrained: false\n    output_stride: 16\n  dropout: 0.1\n\ntraining:\n  epochs: 120\n  batch_size: 4\n  learning_rate: 0.0005\n  weight_decay: 0.0001\n  optimizer: adamw\n  scheduler:\n    type: cosine\n    t_initial: 10\n    eta_min: 0.00001\n  gradient_clip_norm: 1.0\n  num_workers: 4\n  mixed_precision: true\n  checkpoint_dir: outputs/segmentation\n  resume_from: null\n  validate_interval: 1\n  log_interval: 10\n\nloss:\n  name: focal_tversky\n  focal_gamma: 1.5\n  tversky_alpha: 0.7\n  tversky_beta: 0.3\n\nmetrics:\n  evaluate_thresholds: [0.5]\n\nwandb:\n  enabled: false\n  project: isds-traffic-ai\n  run_name: null\n  entity: null\n  tags: [segmentation]\n  notes: null\n  watch: true\n  watch_log: gradients\n  watch_log_freq: 200\n\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: tests/__init__.py\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: tests/test_congestion_pipeline.py\nfrom src.pipeline.congestion import (\n    CongestionClassifier,\n    LoSThresholds,\n    PipelineResult,\n    WindowAggregator,\n)\nfrom src.pipeline.roi import ROILine\n\n\ndef test_congestion_classifier_levels():\n    thresholds = LoSThresholds(\n        density={\"A\": 5, \"B\": 10, \"C\": 14, \"D\": 24, \"E\": 40},\n        speed_two_wheel={\"A\": 40, \"B\": 35, \"C\": 30, \"D\": 25, \"E\": 20, \"F\": 0},\n        speed_four_wheel={\"A\": 45, \"B\": 40, \"C\": 35, \"D\": 25, \"E\": 15, \"F\": 0},\n    )\n    classifier = CongestionClassifier(thresholds)\n    assert classifier.classify(density=4, avg_speed_kph=50, vehicle_type=\"four_wheeler\") == \"A\"\n    assert classifier.classify(density=30, avg_speed_kph=10, vehicle_type=\"two_wheeler\") == \"F\"\n\n\ndef test_window_aggregator_outputs_results():\n    thresholds = LoSThresholds(\n        density={\"A\": 5, \"B\": 10, \"C\": 14, \"D\": 24, \"E\": 40},\n        speed_two_wheel={\"A\": 40, \"B\": 35, \"C\": 30, \"D\": 25, \"E\": 20, \"F\": 0},\n        speed_four_wheel={\"A\": 45, \"B\": 40, \"C\": 35, \"D\": 25, \"E\": 15, \"F\": 0},\n    )\n    classifier = CongestionClassifier(thresholds)\n    roi_lines = [ROILine(y=50.0, x_left=0.0, x_right=100.0, pixel_length=100.0)]\n    aggregator = WindowAggregator(\n        roi_lines=roi_lines,\n        meter_per_pixel=0.01,\n        classifier=classifier,\n        window_seconds=30,\n        min_objects=1,\n        vehicle_type_map={0: \"two_wheeler\"},\n    )\n\n    result = aggregator.record(0, class_id=0, speed_kph=42.0, timestamp=31.0)\n    assert isinstance(result, PipelineResult)\n    lane = result.lanes[0]\n    assert lane.vehicle_count == 1\n    assert lane.level_of_service in {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: tests/test_roi_extractor.py\nimport numpy as np\nimport cv2\n\nfrom src.pipeline.roi import LaneROIExtractor\n\n\ndef test_roi_extraction_produces_lines():\n    mask = np.zeros((120, 200), dtype=np.uint8)\n    cv2.rectangle(mask, (40, 20), (50, 70), 255, -1)\n    cv2.rectangle(mask, (150, 25), (160, 75), 255, -1)\n    cv2.rectangle(mask, (42, 90), (52, 110), 255, -1)\n    cv2.rectangle(mask, (148, 88), (158, 108), 255, -1)\n\n    extractor = LaneROIExtractor(dash_lengths_m=(1.0,), y_group_tolerance_px=20, min_component_area=10)\n    lines, meter_per_pixel = extractor.extract(mask)\n\n    assert len(lines) >= 1\n    assert meter_per_pixel > 0\n    for line in lines:\n        assert line.x_left < line.x_right\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# File: tests/test_segmentation_model.py\nimport torch\n\nfrom src.models.segmentation.deeplab_seresnet import build_segmentation_model\n\n\ndef test_deeplab_seresnet_forward():\n    model = build_segmentation_model(num_classes=2)\n    dummy = torch.randn(2, 3, 256, 256)\n    outputs = model(dummy)\n    assert \"out\" in outputs\n    out = outputs[\"out\"]\n    assert out.shape == (2, 2, 256, 256)\n    out.mean().backward()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Usage Tips**\n- Run the code cells sequentially to define all modules in the notebook session.\n- Use the YAML reference cells to load or tweak configuration dynamically (e.g. via `yaml.safe_load`).\n- Training and pipeline scripts can be invoked directly once prerequisites (datasets, weights, dependencies) are available.\n- The final test cells can be run as quick smoke tests to validate key components.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
